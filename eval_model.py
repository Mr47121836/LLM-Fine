import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel, PeftConfig
import evaluate
import re
from tqdm import tqdm
import argparse

# ------------------ å‘½ä»¤è¡Œå‚æ•° ------------------
parser = argparse.ArgumentParser(description="Qwen3 æ¨¡å‹éªŒè¯è„šæœ¬ - æ”¯æŒ LoRA ä¸å…¨å‚æ•°å¾®è°ƒ")
parser.add_argument("--model_type", type=str, default="lora",
                    choices=["lora", "full"],
                    help="æ¨¡å‹ç±»å‹ï¼š'lora' æˆ– 'full'")
parser.add_argument("--checkpoint_path", type=str, required=True,
                    help="æ£€æŸ¥ç‚¹è·¯å¾„")
parser.add_argument("--test_file", type=str, default="val_format.jsonl",
                    help="æµ‹è¯•é›†è·¯å¾„ï¼ˆæ ¼å¼åŒ–åçš„ jsonlï¼‰")
parser.add_argument("--max_samples", type=int, default=1024,
                    help="è¯„ä¼°çš„æœ€å¤§æ ·æœ¬æ•°")
parser.add_argument("--max_new_tokens", type=int, default=1024,
                    help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
parser.add_argument("--system_prompt", type=str, default=None,
                    help="è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯")
parser.add_argument("--judge_model", type=str, default="Qwen/Qwen2.5-7B-Instruct",
                    help="ç”¨äº G-Eval æ‰“åˆ†çš„å¤§æ¨¡å‹ï¼ˆå»ºè®® 7B ä»¥ä¸Šï¼‰")

args = parser.parse_args()

# ç³»ç»Ÿæç¤ºè¯ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
DEFAULT_SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è®¡ç®—æœºç½‘ç»œä¸“å®¶æ•™æˆï¼Œå…·æœ‰ä¸°å¯Œçš„æ•™å­¦å’Œç ”ç©¶ç»éªŒã€‚"
    "ä½ éœ€è¦é’ˆå¯¹ç”¨æˆ·æå‡ºçš„è®¡ç®—æœºç½‘ç»œç›¸å…³é—®é¢˜ï¼Œå…ˆè¿›è¡Œä¸€æ­¥æ­¥çš„æ€è€ƒï¼Œç„¶åç»™å‡ºå‡†ç¡®ã€è¯¦ç»†ä¸”æ˜“æ‡‚çš„å›ç­”ã€‚"
)

SYSTEM_PROMPT = args.system_prompt if args.system_prompt else DEFAULT_SYSTEM_PROMPT

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ==================== åŠ è½½è¢«è¯„ä¼°æ¨¡å‹ä¸ Tokenizer ====================
print("æ­£åœ¨åŠ è½½è¢«è¯„ä¼°æ¨¡å‹ï¼Œè¯·ç¨ç­‰...")

base_model_name = "Qwen/Qwen3-1.7B"  # â† è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹

tokenizer = AutoTokenizer.from_pretrained(
    base_model_name,
    use_fast=False,
    trust_remote_code=True
)

model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

if args.model_type.lower() == "lora":
    print("åŠ è½½ LoRA å¾®è°ƒæ¨¡å‹...")
    peft_config = PeftConfig.from_pretrained(args.checkpoint_path)
    model = PeftModel.from_pretrained(model, args.checkpoint_path, device_map="auto")
else:
    print("åŠ è½½å…¨å‚æ•°å¾®è°ƒæ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        args.checkpoint_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )

model.eval()
print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼ç±»å‹: {args.model_type} | è®¾å¤‡: {DEVICE}\n")

# ==================== åŠ è½½ G-Eval è¯„å§”æ¨¡å‹ ====================
print("æ­£åœ¨åŠ è½½ G-Eval è¯„å§”æ¨¡å‹ï¼ˆå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
judge_tokenizer = AutoTokenizer.from_pretrained(args.judge_model, trust_remote_code=True)
judge_model = AutoModelForCausalLM.from_pretrained(
    args.judge_model,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)
judge_pipe = pipeline(
    "text-generation",
    model=judge_model,
    tokenizer=judge_tokenizer,
    device_map="auto"
)
print("è¯„å§”æ¨¡å‹åŠ è½½å®Œæˆ\n")

# ==================== åŠ è½½æµ‹è¯•é›† ====================
with open(args.test_file, "r", encoding="utf-8") as f:
    test_data = [json.loads(line) for line in f if line.strip()]

print(f"åŠ è½½äº† {len(test_data)} æ¡æµ‹è¯•æ•°æ®ï¼Œå°†è¯„ä¼°å‰ {args.max_samples} æ¡\n")

# ==================== åŠ è½½è¯„ä¼°æŒ‡æ ‡ ====================
rouge = evaluate.load("rouge")
bertscore = evaluate.load("bertscore")
chrf = evaluate.load("chrf")  # ç”¨äºè®¡ç®— ChrF++

# ==================== G-Eval æç¤ºæ¨¡æ¿ï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰ ====================
GEVAL_PROMPT = """\
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¯„ä¼°ä¸“å®¶ã€‚
è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†ï¼Œå¯¹æ¨¡å‹ç”Ÿæˆçš„å›ç­”è¿›è¡Œè¯„åˆ†ï¼ˆæ»¡åˆ† 10 åˆ†ï¼‰ï¼š

è¯„åˆ†æ ‡å‡†ï¼š
1. å‡†ç¡®æ€§ï¼šå›ç­”æ˜¯å¦æ­£ç¡®ã€æ²¡æœ‰äº‹å®é”™è¯¯ï¼ˆæƒé‡æœ€é«˜ï¼‰
2. å®Œæ•´æ€§ï¼šæ˜¯å¦æ¶µç›–äº†é—®é¢˜çš„ä¸»è¦æ–¹é¢
3. é€»è¾‘æ€§ä¸æ€è€ƒè¿‡ç¨‹ï¼šæ˜¯å¦æœ‰æ¸…æ™°çš„ <think>...</think> æ€è€ƒæ­¥éª¤
4. å¯è¯»æ€§ä¸ä¸“ä¸šæ€§ï¼šè¯­è¨€æ˜¯å¦ä¸“ä¸šã€æ¸…æ™°ã€æ˜“æ‡‚

é—®é¢˜ï¼š{question}

å‚è€ƒç­”æ¡ˆï¼š{reference}

æ¨¡å‹ç”Ÿæˆå›ç­”ï¼š{response}

è¯·ç›´æ¥ç»™å‡ºä¸€ä¸ª 0~10 çš„æ•´æ•°åˆ†æ•°ï¼ˆåªè¾“å‡ºæ•°å­—ï¼Œä¸è¦è§£é‡Šï¼‰ï¼Œè¶Šé«˜ä»£è¡¨æ•´ä½“è´¨é‡è¶Šå¥½ã€‚
"""

# ==================== å¼€å§‹è¯„ä¼° ====================
think_count = 0
results = []
eval_samples = min(args.max_samples, len(test_data))

for item in tqdm(test_data[:eval_samples], desc="è¯„ä¼°è¿›åº¦", unit="æ¡"):
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": item["input"]}
    ]
    
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer([text], return_tensors="pt").to(DEVICE)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=args.max_new_tokens,
            temperature=0.7,
            do_sample=True,
            repetition_penalty=1.1
        )
    
    response = tokenizer.decode(
        outputs[0][inputs['input_ids'].shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    # æ˜¯å¦åŒ…å«æ€è€ƒè¿‡ç¨‹
    has_think = bool(re.search(r"<think>.*?</think>", response, re.DOTALL))
    if has_think:
        think_count += 1
    
    reference = item["output"]
    question = item["input"]
    
    # ChrF++ (word_order=2 å³ ChrF++)
    chrf_result = chrf.compute(
        predictions=[response],
        references=[[reference]],
        char_order=6,
        word_order=2,
        beta=2
    )
    chrf_pp = chrf_result['score'] / 100.0  # è½¬ä¸º 0~1 èŒƒå›´ï¼Œæ–¹ä¾¿æ¯”è¾ƒ
    
    # ROUGE-L
    rouge_result = rouge.compute(predictions=[response], references=[reference])
    rouge_l = rouge_result["rougeL"]
    
    # BERTScore
    bert_result = bertscore.compute(predictions=[response], references=[reference], lang="zh")
    bert_f1 = sum(bert_result["f1"]) / len(bert_result["f1"])
    
    # G-Eval åˆ†æ•°ï¼ˆä½¿ç”¨å¼ºæ¨¡å‹æ‰“åˆ†ï¼‰
    geval_input = GEVAL_PROMPT.format(
        question=question,
        reference=reference,
        response=response
    )
    
    try:
        judge_output = judge_pipe(
            geval_input,
            max_new_tokens=32,
            temperature=0.1,
            do_sample=False,
            num_return_sequences=1
        )[0]['generated_text'].strip()
        
        # å°è¯•æå–æ•°å­—åˆ†æ•°
        geval_score = 0.0
        for token in judge_output.split():
            try:
                score = float(token)
                if 0 <= score <= 10:
                    geval_score = score
                    break
            except:
                pass
        geval_score = geval_score / 10.0  # è½¬ä¸º 0~1
    except:
        geval_score = 0.0  # é˜²é”™å¤„ç†

    results.append({
        "question": question,
        "response": response,
        "has_think": has_think,
        "chrf_pp": chrf_pp,
        "rougeL": rouge_l,
        "bertscore": bert_f1,
        "geval": geval_score
    })

# ==================== è¾“å‡ºç»“æœ ====================
if results:
    think_rate = think_count / len(results) * 100
    avg_chrf = sum(r["chrf_pp"] for r in results) / len(results)
    avg_rouge = sum(r["rougeL"] for r in results) / len(results)
    avg_bert = sum(r["bertscore"] for r in results) / len(results)
    avg_geval = sum(r["geval"] for r in results) / len(results)
else:
    think_rate = avg_chrf = avg_rouge = avg_bert = avg_geval = 0.0

print("\n" + "="*80)
print(f"è¯„ä¼°å®Œæˆï¼ï¼ˆæ ·æœ¬æ•°ï¼š{len(results)}ï¼‰")
print(f"æ¨¡å‹ç±»å‹       : {args.model_type.upper()}")
print(f"æ£€æŸ¥ç‚¹è·¯å¾„     : {args.checkpoint_path}")
print(f"Think Rateï¼ˆæ€è€ƒç‡ï¼‰ : {think_rate:6.2f}%  ({think_count}/{len(results)})")
print(f"Avg ChrF++           : {avg_chrf:.4f}")
print(f"Average ROUGE-L      : {avg_rouge:.4f}")
print(f"Average BERTScore    : {avg_bert:.4f}")
print(f"Average G-Eval (0-1) : {avg_geval:.4f}")
print("="*80)

if avg_geval >= 0.85 and avg_chrf > 0.75:
    print("ğŸ‰ æ¨¡å‹è¡¨ç°éå¸¸ä¼˜ç§€ï¼å¯ä»¥è€ƒè™‘æ­£å¼éƒ¨ç½²ï¼")
elif avg_geval >= 0.70 and avg_chrf > 0.60:
    print("ğŸ‘ æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼ŒåŸºæœ¬å¯ç”¨ï¼Œå¯ç»§ç»­è§‚å¯Ÿæˆ–è½»å¾®ä¼˜åŒ–")
else:
    print("âš  æ¨¡å‹è¡¨ç°ä¸€èˆ¬ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡ã€æç¤ºè¯æˆ–è®­ç»ƒè®¾ç½®")

"""
éªŒè¯ LoRA æ¨¡å‹python eval_model.py --model_type lora --checkpoint_path output/Qwen3-1.7B-network-lora/checkpoint-321 --test_file datasets/val_format.jsonl --max_samples 30"""
"""
éªŒè¯å…¨å‚æ•°å¾®è°ƒæ¨¡å‹
python eval_model.py --model_type full --checkpoint_path output/Qwen3-1.7B-network/checkpoint-426 --test_file datasets/val_format.jsonl --max_samples 30
"""

